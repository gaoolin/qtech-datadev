server:
  port: 8081
  servlet:
    context-path: /
  tomcat:
    uri-encoding: UTF-8
    accept-count: 1000
    threads:
      max: 1000
      min-spare: 10

spring:
  application:
    name: aa-params-chk-service
    version: 1.0.0

  profiles:
    active: dev

  mvc:
    pathmatch:
      matching-strategy: ant-path-matcher

  #redis配置信息
  redis:
    ## Redis数据库索引（默认为0）
    database: 0
    ## Redis服务器连接密码（默认为空）
    password: im@2024
    ## 连接超时时间（毫秒）
    timeout: 5000
    ## 集群配置
    cluster:
      ### 集群中所有节点
      nodes:
        - 10.170.6.24:6379
        - 10.170.6.25:6379
        - 10.170.6.26:6379
        - 10.170.6.141:6379
        - 10.170.6.142:6379
      ### 最大重定向数，最好为集群节点数，比如第一台挂了，连第二台，第二台挂了连第三台，这个是重新连接的最大数量
      max-redirects: 6
    lettuce:
      pool:
        ## 连接池最大连接数（使用负值表示没有限制）
        max-active: 8
        ## 连接池最大阻塞等待时间（使用负值表示没有限制）
        max-wait: -1
        ## 连接池中的最大空闲连接
        max-idle: 8
        ## 连接池中的最小空闲连接
        min-idle: 1
      ## 集群配置
      cluster:
        refresh:
          # 支持集群拓扑动态感应刷新,自适应拓扑刷新是否使用所有可用的更新，默认false关闭，类似nacos定时刷新服务列表
          adaptive: true
          # 定时刷新时间 毫秒
          period: 2000

  kafka:
    bootstrap-servers:
      - 10.170.6.24:9092
      - 10.170.6.25:9092
      - 10.170.6.26:9092
    #生产者
    producer:
      #客户端发送服务端失败的重试次数
      retries: 2
      #多个记录被发送到同一个分区时,生产者将尝试将记录一起批处理成更少的请求.
      #此设置有助于提高客户端和服务器的性能,配置控制默认批量大小(以字节为单位)
      batch-size: 16384
      #生产者可用于缓冲等待发送到服务器的记录的总内存字节数(以字节为单位)
      buffer-memory: 33554432
      #指定key使用的序列化类
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      #指定value使用的序列化类
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      #生产者producer要求leader节点在考虑完成请求之前收到的确认数,用于控制发送记录在服务端的持久化
      #acks=0,设置为0,则生产者producer将不会等待来自服务器的任何确认.该记录将立即添加到套接字(socket)缓冲区并视为已发送.在这种情况下,无法保证服务器已收到记录,并且重试配置(retries)将不会生效(因为客户端通常不会知道任何故障),每条记录返回的偏移量始终设置为-1.
      #acks=1,设置为1,leader节点会把记录写入本地日志,不需要等待所有follower节点完全确认就会立即应答producer.在这种情况下,在follower节点复制前,leader节点确认记录后立即失败的话,记录将会丢失.
      #acks=all,acks=-1,leader节点将等待所有同步复制副本完成再确认记录,这保证了只要至少有一个同步复制副本存活,记录就不会丢失.
      acks: -1
    consumer:
      group-id: aaList-default-group
      client-id: spring-boot-aaList-client
      #开启consumer的偏移量(offset)自动提交到Kafka
      enable-auto-commit: true
      #consumer的偏移量(offset)自动提交的时间间隔,单位毫秒
      auto-commit-interval: 1000
      #在Kafka中没有初始化偏移量或者当前偏移量不存在情况
      #earliest,在偏移量无效的情况下,自动重置为最早的偏移量
      #latest,在偏移量无效的情况下,自动重置为最新的偏移量
      #none,在偏移量无效的情况下,抛出异常.
      auto-offset-reset: earliest
      #一次调用poll返回的最大记录条数
      max-poll-records: 500
      #请求阻塞的最大时间(毫秒)
      fetch-max-wait: 500
      #请求应答的最小字节数
      fetch-min-size: 1
      #心跳间隔时间(毫秒)
      heartbeat-interval: 3000
      #指定key使用的反序列化类
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      #指定value使用的反序列化类
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

  rabbitmq:
    host: 10.170.6.40
    port: 31131
    username: qtech
    password: Ee786549


mybatis:
  type-aliases-package: com.qtech.check.pojo

management:
  endpoints:
    web:
      exposure:
        include: '*'
  endpoint:
    health:
      show-details: always
  metrics:
    tags:
      instance: ${spring.application.name}-${spring.application.instance_id:${random.value}}
      version: ${spring.application.version}
      application: ${spring.application.name}
    export:
      prometheus:
        enabled: true

logging:
  # 若不配置，默认使用 ##logback-spring.xml，logback-udf.xml不能命名为 logback-spring.xml，logback.xml的加载早于application.yml配置文件，在logback.xml中使用<springProperty>标签来获取配置文件中的变量值，读取不到而报出错误信息。
  config: classpath:logback-spring.xml
